# summarize-logic.py ‚Äì v3.9 (Improved Tailwind Content Audit)
"""
CLI tool to generate a logic map of the This-Is-Us repo and surface
common Town‚ÄØHall wiring mistakes, with enhanced code analysis.

Improvements in v3.9
--------------------
* **Smarter Tailwind Content Path Audit**: The `--audit-tailwind` function now
  strips JavaScript comments from the content array before parsing paths.
  This prevents errors and provides a much more accurate report on
  which files Tailwind is configured to scan.

Usage examples
--------------
```bash
# ONLY run the new, more accurate Tailwind content path audit
python summarize-logic.py --audit-tailwind
```
"""
from __future__ import annotations

import argparse
import os
import re
import sys
from collections import defaultdict
from pathlib import Path
from typing import List, Set, Dict, Any
from glob import glob

try:
    import frontmatter  # type: ignore
except ImportError:
    print("‚ö†Ô∏è  pip install python-frontmatter for full feature support", file=sys.stderr)
    frontmatter = None  # fallback safe-guard

BASE_DIR = Path('.')
TARGET_EXTENSIONS = {'.js', '.html', '.mjs', '.py', '.md', '.toml', '.sql'}
IGNORED_DIRS = {
    'node_modules', '__pycache__', '.git', 'venv', 'static/admin', 'themes', 'public',
    'dist', 'build', '.wrangler', '.parcel-cache', '.idea', '.vscode', '.pytest_cache',
}
IGNORED_EXTENSIONS = {'.bak', '.tmp', '.log'}

TOWNHALL_EXPECTED_LAYOUTS = [
    'layouts/townhall/list.html',
    'layouts/townhall/thread.html',
    'layouts/townhall/create.html',
    'layouts/section/townhall.html',
]
TOWNHALL_EXPECTED_JS = [
    'static/js/townhall/home.js',
    'static/js/townhall/threads.js',
    'static/js/townhall/thread-view.js',
]

# Regex patterns for various code elements
RE_DEFINITIONS = re.compile(
    r'^\s*(def|class|function|const\s+\w+\s*=|let\s+\w+\s*=|var\s+\w+\s*=)\s*(\w+)',
    re.MULTILINE
)
RE_JS_IMPORTS_EXPORTS = re.compile(
    r'(?:import(?:["\'\s]*(?:[\w*{}\n\r\t, ]+)\s*from\s*)?["\'`](.*?)["\'`])|'
    r'(?:require\s*\(\s*["\'`](.*?)["\'`]\s*\))|'
    r'(?:export(?:\s+default)?(?:\s+(?:function|class|const|let|var))?\s*(\w+))',
    re.MULTILINE
)
RE_PY_IMPORTS = re.compile(r'^\s*(?:from\s+([\w.]+)\s+)?import\s+([\w.,\s]+)', re.MULTILINE)
RE_API_ENDPOINTS = re.compile(
    r'(?:router|app|proxy)\.(get|post|put|delete|patch|options)\s*\(\s*["\'`](.*?)["\'`]',
    re.MULTILINE | re.IGNORECASE
)
RE_HUGO_PARTIALS = re.compile(r'\{\{\s*(?:partial|template)\s+["\'](.+?)["\']', re.MULTILINE)
RE_HTML_SCRIPT_SRC = re.compile(r'<script\s+[^>]*?src=["\'](.+?)["\']', re.MULTILINE)
RE_HTML_LINK_HREF = re.compile(r'<link\s+[^>]*?href=["\'](.+?)["\']', re.MULTILINE)
RE_TAILWIND_CONTENT = re.compile(r'content\s*:\s*\[([\s\S]*?)\]', re.MULTILINE) # More robust regex


# ANSI helpers
RED = '\u001b[31m'; YELLOW = '\u001b[33m'; GREEN = '\u001b[32m'; RESET = '\u001b[0m'

def colour(text: str, col: str) -> str:
    return f"{col}{text}{RESET}"

def extract_summary(path: Path) -> str:
    """Extracts a brief summary from the beginning of the file."""
    try:
        lines = path.read_text(encoding='utf-8', errors='ignore').splitlines()
        comments = [ln.strip() for ln in lines if ln.strip().startswith(('//', '#', '--', '/*', '---'))]
        if not comments and path.suffix == '.md' and frontmatter:
            try:
                post = frontmatter.load(path)
                if 'title' in post.metadata:
                    return f"Title: {post.metadata['title']}"
                if 'description' in post.metadata:
                    return f"Description: {post.metadata['description']}"
            except Exception:
                pass # Fallback to no summary if frontmatter fails
        return ' '.join(comments[:5]) if comments else '(no summary)'
    except Exception:
        return '(unreadable)'

def analyze_file_code(path: Path) -> Dict[str, Any]:
    """Analyzes file content for definitions, imports/exports, and API endpoints."""
    results: Dict[str, Any] = defaultdict(list)
    try:
        code = path.read_text(encoding='utf-8', errors='ignore')

        # Definitions
        for match in RE_DEFINITIONS.finditer(code):
            name = match.group(2)
            if name and name not in results['definitions']:
                results['definitions'].append(name)
        
        # Imports/Exports
        if path.suffix in {'.js', '.mjs'}:
            for match in RE_JS_IMPORTS_EXPORTS.finditer(code):
                if match.group(1):
                    results['imports_exports'].append(f"import '{match.group(1)}'")
                elif match.group(2):
                    results['imports_exports'].append(f"require('{match.group(2)}')")
                elif match.group(3):
                    results['imports_exports'].append(f"export {match.group(3)}")
        elif path.suffix == '.py':
            for match in RE_PY_IMPORTS.finditer(code):
                from_path = match.group(1)
                imports = match.group(2).split(',')
                for imp in imports:
                    if from_path:
                        results['imports_exports'].append(f"from {from_path} import {imp.strip()}")
                    else:
                        results['imports_exports'].append(f"import {imp.strip()}")

        # API Endpoints
        if path.suffix in {'.js', '.mjs'} and any(p in path.parts for p in ['worker/src', 'mcp/src', 'ballots/src']):
            for match in RE_API_ENDPOINTS.finditer(code):
                results['api_endpoints'].append(f"{match.group(1).upper()} {match.group(2)}")
        
        # Hugo Partials/Templates & HTML links
        if path.suffix == '.html':
            for match in RE_HUGO_PARTIALS.finditer(code):
                results['hugo_partials'].append(match.group(1))
            for match in RE_HTML_SCRIPT_SRC.finditer(code):
                results['html_script_src'].append(match.group(1))
            for match in RE_HTML_LINK_HREF.finditer(code):
                results['html_link_href'].append(match.group(1))

    except Exception:
        results = {k: ['(unreadable)'] for k in results.keys()}

    # Ensure sorted unique lists
    for key in results:
        results[key] = sorted(list(set(results[key])))
    if not results['definitions']: results['definitions'] = ['(no defs)']
    if not results['imports_exports']: results['imports_exports'] = ['(no imports/exports)']
    
    return results

def should_skip(path: Path) -> bool:
    """Determines if a path should be skipped based on ignored directories and extensions."""
    return any(part in IGNORED_DIRS for part in path.parts) or path.suffix.lower() in IGNORED_EXTENSIONS

def walk_directory(base: Path, capture: List[str]):
    """Recursively walks the directory, analyzes files, and captures summary."""
    for root, dirs, files in os.walk(base):
        dirs[:] = [d for d in dirs if d not in IGNORED_DIRS]
        for fname in files:
            fpath = Path(root) / fname
            if should_skip(fpath):
                continue
            if fpath.suffix.lower() in TARGET_EXTENSIONS:
                rel = fpath.relative_to(base)
                analysis = analyze_file_code(fpath)
                
                capture.append(f"\n### `{rel}`\n")
                capture.append(f"**Summary**: {extract_summary(fpath)}\n")
                capture.append("**Definitions**:\n" + '\n'.join(f"- `{d}`" for d in analysis['definitions']))
                
                if analysis.get('imports_exports') and analysis['imports_exports'] != ['(no imports/exports)']:
                    capture.append("\n**Imports/Exports**:\n" + '\n'.join(f"- `{ie}`" for ie in analysis['imports_exports']))
                for key in ['api_endpoints', 'hugo_partials', 'html_script_src', 'html_link_href']:
                    if analysis.get(key):
                        title = key.replace('_', ' ').title().replace('Html', 'HTML')
                        capture.append(f"\n**{title}**:\n" + '\n'.join(f"- `{item}`" for item in analysis[key]))

def audit_townhall(capture: List[str]):
    """Performs specific audits for Town Hall features."""
    capture.append("\n## üîç Town‚ÄØHall Debug Audit\n")
    capture.append("### Layout templates\n")
    for rel in TOWNHALL_EXPECTED_LAYOUTS:
        status = colour('‚úÖ found', GREEN) if Path(rel).exists() else colour('‚ùå missing', RED)
        capture.append(f"- `{rel}` ‚Äì {status}")

    capture.append("\n### Critical JS files\n")
    for rel in TOWNHALL_EXPECTED_JS:
        status = colour('‚úÖ found', GREEN) if Path(rel).exists() else colour('‚ùå missing', RED)
        capture.append(f"- `{rel}` ‚Äì {status}")

def audit_tailwind_content_paths(capture: List[str]):
    """
    **NEW**: Audits the content paths in tailwind.config.js to ensure they match files.
    This is a critical diagnostic for CSS compilation issues.
    """
    capture.append("\n## üå™Ô∏è Tailwind Content Path Audit\n")
    config_path = Path('tailwind.config.js')
    if not config_path.exists():
        capture.append(colour(f"‚ùå `tailwind.config.js` not found!", RED))
        return

    try:
        content = config_path.read_text(encoding='utf-8')
        match = RE_TAILWIND_CONTENT.search(content)
        if not match:
            capture.append(colour("‚ùå Could not find `content: [...]` array in config.", RED))
            return
        
        # Extract content block and clean it
        patterns_str = match.group(1)
        # Remove lines containing JavaScript comments
        cleaned_patterns_str = '\n'.join(line for line in patterns_str.splitlines() if not line.strip().startswith('//'))
        
        # Extract comma-separated, quoted strings from the cleaned content block
        patterns = [p.strip().strip("'\"") for p in cleaned_patterns_str.split(',') if p.strip()]
        
        capture.append(f"Found {len(patterns)} content patterns to check in `{config_path}`:\n")

        total_files_found = 0
        for pattern in patterns:
            if not pattern: continue
            # The glob function needs recursive=True for '**'
            found_files = glob(pattern, recursive=True)
            count = len(found_files)
            total_files_found += count
            status = colour(f"‚úÖ Found {count} files", GREEN) if count > 0 else colour(f"‚ùå Found 0 files", RED)
            capture.append(f"- Pattern: `{pattern}` ‚Üí {status}")

        capture.append("\n" + ("-"*20))
        if total_files_found > 0:
            capture.append(colour(f"‚úÖ Success! The patterns match a total of {total_files_found} files.", GREEN))
        else:
            capture.append(colour(f"‚ùå Critical Error! No files were found by any content pattern.", RED))
            capture.append(colour("   This is why your CSS is not compiling. Check the paths above.", RED))


    except Exception as e:
        capture.append(colour(f"An error occurred during Tailwind audit: {e}", RED))

def generate_report_section(title: str, items: List[str], capture: List[str]):
    """Helper to generate a markdown section for a list of items."""
    capture.append(f"\n## {title}\n")
    if items:
        for item in sorted(list(set(items))):
            capture.append(f"- {item}")
    else:
        capture.append(colour(f"No items detected for this section.", YELLOW))

def main():
    parser = argparse.ArgumentParser(description="Generate logic index & project audits.")
    parser.add_argument('--audit-tailwind', action='store_true', help='Only run the Tailwind content path audit.')
    parser.add_argument('--townhall-only', action='store_true', help='Only run Town‚ÄØHall audit.')
    parser.add_argument('--skip-index', action='store_true', help='Skip file index generation.')
    parser.add_argument('--output', default='logic-index-v3.md', help='Path or - for stdout')
    args = parser.parse_args()

    capture: List[str] = ["# Logic Index ‚Äì This Is Us Project (v3.9)\n"]

    if args.audit_tailwind:
        audit_tailwind_content_paths(capture)
    else:
        if not args.townhall_only and not args.skip_index:
            walk_directory(BASE_DIR, capture)

        audit_townhall(capture)
        
        # Simplified report generation
        all_items: Dict[str, List[str]] = defaultdict(list)
        for root, _, files in os.walk(BASE_DIR):
             for fname in files:
                fpath = Path(root) / fname
                if should_skip(fpath) or fpath.suffix not in {'.html', '.js', '.mjs'}:
                    continue
                analysis = analyze_file_code(fpath)
                if analysis.get('api_endpoints'):
                    all_items['üåê API Endpoints Found'].extend(analysis['api_endpoints'])
                if analysis.get('hugo_partials'):
                    all_items['üìÑ Hugo Template Usage'].extend([f"`{hp}` called in `{fpath.relative_to(BASE_DIR)}`" for hp in analysis['hugo_partials']])
                if analysis.get('html_script_src'):
                    all_items['üîó External JavaScript Includes'].extend([f"`{src}` in `{fpath.relative_to(BASE_DIR)}`" for src in analysis['html_script_src']])
                if analysis.get('html_link_href'):
                    all_items['üé® External Stylesheet Includes'].extend([f"`{href}` in `{fpath.relative_to(BASE_DIR)}`" for href in analysis['html_link_href']])

        for title, items in all_items.items():
            generate_report_section(title, items, capture)

    output_text = '\n'.join(capture)

    if args.output == '-':
        print(output_text)
    else:
        Path(args.output).write_text(output_text, encoding='utf-8')
        print(colour(f"‚úÖ {args.output} generated", GREEN))

if __name__ == '__main__':
    main()
